"""
DevFeed ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®ã‚µã‚¤ãƒˆã‹ã‚‰æœ€æ–°ã®æŠ€è¡“è¨˜äº‹ã‚’å–å¾—ã—ã¾ã™ï¼š
- Zenn (RSS)
- Qiita (å…¬å¼API)
- ã¯ã¦ãªãƒ–ãƒ­ã‚° (RSS)

å–å¾—ã—ãŸãƒ‡ãƒ¼ã‚¿ã¯ data/articles.json ã«ä¿å­˜ã•ã‚Œã¾ã™ã€‚
"""

import json
import os
from datetime import datetime
from typing import List, Dict
import requests
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET


class ArticleScraper:
    """è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦åé›†ã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.articles: List[Dict] = []
    
    def fetch_zenn_articles(self, limit: int = 10) -> List[Dict]:
        """
        Zennã®RSSãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
        RSS URL: https://zenn.dev/feed
        """
        print("ğŸ“š Zennã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ä¸­...")
        try:
            response = requests.get("https://zenn.dev/feed", timeout=10)
            response.raise_for_status()
            
            # XMLã‚’ãƒ‘ãƒ¼ã‚¹
            root = ET.fromstring(response.content)
            articles = []
            
            # RSS 2.0å½¢å¼
            for item in root.findall(".//item")[:limit]:
                title = item.find("title")
                link = item.find("link")
                pub_date = item.find("pubDate")
                description = item.find("description")
                
                if title is not None and link is not None:
                    article = {
                        "id": f"zenn_{link.text.split('/')[-1]}",
                        "title": title.text,
                        "url": link.text,
                        "source": "Zenn",
                        "publishedAt": self._parse_date(pub_date.text if pub_date is not None else ""),
                        "description": self._clean_html(description.text if description is not None else "")
                    }
                    articles.append(article)
            
            print(f"âœ… Zennã‹ã‚‰ {len(articles)} ä»¶å–å¾—")
            return articles
            
        except Exception as e:
            print(f"âŒ Zennã®å–å¾—ã«å¤±æ•—: {e}")
            return []
    
    def fetch_qiita_articles(self, limit: int = 10) -> List[Dict]:
        """
        Qiita APIã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
        API URL: https://qiita.com/api/v2/items
        """
        print("ğŸ“š Qiitaã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ä¸­...")
        try:
            headers = {
                "User-Agent": "DevFeed/1.0"
            }
            # å…¬é–‹APIã‚’ä½¿ç”¨ï¼ˆèªè¨¼ä¸è¦ï¼‰
            response = requests.get(
                "https://qiita.com/api/v2/items",
                params={"page": 1, "per_page": limit},
                headers=headers,
                timeout=10
            )
            response.raise_for_status()
            
            data = response.json()
            articles = []
            
            for item in data:
                article = {
                    "id": f"qiita_{item['id']}",
                    "title": item["title"],
                    "url": item["url"],
                    "source": "Qiita",
                    "publishedAt": item["created_at"][:10],  # YYYY-MM-DDå½¢å¼ã«
                    "description": item.get("body", "")[:150] + "..."  # æœ€åˆã®150æ–‡å­—
                }
                articles.append(article)
            
            print(f"âœ… Qiitaã‹ã‚‰ {len(articles)} ä»¶å–å¾—")
            return articles
            
        except Exception as e:
            print(f"âŒ Qiitaã®å–å¾—ã«å¤±æ•—: {e}")
            return []
    
    def fetch_hatena_articles(self, limit: int = 10) -> List[Dict]:
        """
        ã¯ã¦ãªãƒ–ãƒ­ã‚°ã®ãƒ›ãƒƒãƒˆã‚¨ãƒ³ãƒˆãƒªãƒ¼ï¼ˆãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ï¼‰ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
        RSS URL: https://b.hatena.ne.jp/hotentry/it.rss
        """
        print("ğŸ“š ã¯ã¦ãªãƒ–ãƒ­ã‚°ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—ä¸­...")
        try:
            response = requests.get("https://b.hatena.ne.jp/hotentry/it.rss", timeout=10)
            response.raise_for_status()
            
            # XMLã‚’ãƒ‘ãƒ¼ã‚¹
            root = ET.fromstring(response.content)
            articles = []
            
            # RDFå½¢å¼ã¨RSS 2.0å½¢å¼ã®ä¸¡æ–¹ã«å¯¾å¿œ
            # ã¾ãšRDFå½¢å¼ã‚’è©¦ã™
            items = root.findall(".//{http://purl.org/rss/1.0/}item")
            
            # RDFå½¢å¼ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒãªã„å ´åˆã€RSS 2.0å½¢å¼ã‚’è©¦ã™
            if not items:
                items = root.findall(".//item")
            
            for item in items[:limit]:
                # RDFå½¢å¼ã®ã‚¿ã‚°ã‚’è©¦ã™
                title = item.find("{http://purl.org/rss/1.0/}title")
                link = item.find("{http://purl.org/rss/1.0/}link")
                date = item.find("{http://purl.org/dc/elements/1.1/}date")
                description = item.find("{http://purl.org/rss/1.0/}description")
                
                # RSS 2.0å½¢å¼ã®ã‚¿ã‚°ã‚‚è©¦ã™
                if title is None:
                    title = item.find("title")
                if link is None:
                    link = item.find("link")
                if date is None:
                    date = item.find("pubDate")
                if description is None:
                    description = item.find("description")
                
                if title is not None and link is not None:
                    # ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—ï¼ˆNoneãƒã‚§ãƒƒã‚¯ï¼‰
                    title_text = title.text if title.text else "ã‚¿ã‚¤ãƒˆãƒ«ãªã—"
                    link_text = link.text if link.text else ""
                    
                    # æ—¥ä»˜ã®å‡¦ç†
                    if date is not None and date.text:
                        try:
                            # ISOå½¢å¼ã®æ—¥ä»˜ã®å ´åˆ
                            if 'T' in date.text:
                                published_date = date.text[:10]
                            else:
                                # RFC 2822å½¢å¼ã®æ—¥ä»˜ã®å ´åˆ
                                published_date = self._parse_date(date.text)
                        except:
                            published_date = datetime.now().strftime("%Y-%m-%d")
                    else:
                        published_date = datetime.now().strftime("%Y-%m-%d")
                    
                    # èª¬æ˜æ–‡ã®å‡¦ç†
                    desc_text = description.text if description is not None and description.text else ""
                    
                    article = {
                        "id": f"hatena_{abs(hash(link_text))}",
                        "title": title_text,
                        "url": link_text,
                        "source": "ã¯ã¦ãªãƒ–ãƒ­ã‚°",
                        "publishedAt": published_date,
                        "description": self._clean_html(desc_text) if desc_text else ""
                    }
                    articles.append(article)
            
            print(f"âœ… ã¯ã¦ãªãƒ–ãƒ­ã‚°ã‹ã‚‰ {len(articles)} ä»¶å–å¾—")
            return articles
            
        except Exception as e:
            print(f"âŒ ã¯ã¦ãªãƒ–ãƒ­ã‚°ã®å–å¾—ã«å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _parse_date(self, date_str: str) -> str:
        """æ—¥ä»˜æ–‡å­—åˆ—ã‚’YYYY-MM-DDå½¢å¼ã«å¤‰æ›"""
        try:
            # RFC 2822å½¢å¼ã®æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
            from email.utils import parsedate_to_datetime
            dt = parsedate_to_datetime(date_str)
            return dt.strftime("%Y-%m-%d")
        except:
            return datetime.now().strftime("%Y-%m-%d")
    
    def _clean_html(self, html: str) -> str:
        """HTMLã‚¿ã‚°ã‚’é™¤å»ã—ã¦ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›"""
        if not html:
            return ""
        
        try:
            soup = BeautifulSoup(html, "html.parser")
            text = soup.get_text()
            # æ”¹è¡Œã‚„ä½™åˆ†ãªç©ºç™½ã‚’å‰Šé™¤
            text = " ".join(text.split())
            return text[:200] + "..." if len(text) > 200 else text
        except:
            return ""
    
    def fetch_all(self):
        """ã™ã¹ã¦ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—"""
        print("\nğŸš€ è¨˜äº‹ã®å–å¾—ã‚’é–‹å§‹ã—ã¾ã™...\n")
        
        # å„ã‚½ãƒ¼ã‚¹ã‹ã‚‰è¨˜äº‹ã‚’å–å¾—
        zenn_articles = self.fetch_zenn_articles(limit=10)
        qiita_articles = self.fetch_qiita_articles(limit=10)
        hatena_articles = self.fetch_hatena_articles(limit=10)
        
        # çµ±åˆ
        self.articles = zenn_articles + qiita_articles + hatena_articles
        
        # æ—¥ä»˜ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        self.articles.sort(key=lambda x: x["publishedAt"], reverse=True)
        
        print(f"\nâœ¨ åˆè¨ˆ {len(self.articles)} ä»¶ã®è¨˜äº‹ã‚’å–å¾—ã—ã¾ã—ãŸ\n")
    
    def save_to_json(self, output_path: str = "data/articles.json"):
        """å–å¾—ã—ãŸè¨˜äº‹ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        # dataãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã‘ã‚Œã°ä½œæˆ
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        output_data = {
            "updated_at": datetime.now().isoformat(),
            "total": len(self.articles),
            "articles": self.articles
        }
        
        # JSONå½¢å¼ã§ä¿å­˜
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    scraper = ArticleScraper()
    scraper.fetch_all()
    scraper.save_to_json()
    print("\nğŸ‰ å®Œäº†ï¼\n")


if __name__ == "__main__":
    main()
